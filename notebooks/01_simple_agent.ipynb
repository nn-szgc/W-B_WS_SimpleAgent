{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "585792d2",
      "metadata": {
        "id": "585792d2"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/morganmcg1/deep-research-bot/blob/main/notebooks/01_simple_agent.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85671824",
      "metadata": {
        "id": "85671824"
      },
      "source": [
        "# Introduction to Building LLM Agents with Tools and Tracing\n",
        "\n",
        "<!--- @wandbcode{fc-london-workshop-2025} -->\n",
        "\n",
        "This script walks through the process of building a simple LLM-powered agent that can use tools (functions) to answer questions. We'll cover:\n",
        "1. Making basic LLM calls.\n",
        "2. Introducing Weave for tracing and observability.\n",
        "3. Defining tools for the LLM (manually and automatically).\n",
        "4. Implementing a basic agentic loop.\n",
        "5. Structuring the agent using Python classes.\n",
        "6. Running the agent on a multi-step task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "803fbc4f",
      "metadata": {
        "id": "803fbc4f"
      },
      "source": [
        "**Prerequisites:**\n",
        "Make sure you have the necessary libraries installed:\n",
        "```bash\n",
        "!uv pip install -qqq git+https://github.com/morganmcg1/deep-research-bot.git\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9ddd2e13",
      "metadata": {
        "id": "9ddd2e13"
      },
      "outputs": [],
      "source": [
        "!uv pip install -qqq git+https://github.com/morganmcg1/deep-research-bot.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "96d103af",
      "metadata": {
        "id": "96d103af"
      },
      "outputs": [],
      "source": [
        "# Global Configuration & Setup\n",
        "import os\n",
        "import inspect\n",
        "import json\n",
        "import weave # Must import weave before litellm for auto-patching\n",
        "import openai\n",
        "from enum import Enum\n",
        "from pydantic import BaseModel, Field\n",
        "from rich.panel import Panel\n",
        "from rich.markdown import Markdown\n",
        "from rich.console import Console as RichConsole\n",
        "from exa_py import Exa\n",
        "from typing import Any, Callable, get_type_hints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0c9cf4c",
      "metadata": {
        "id": "f0c9cf4c"
      },
      "source": [
        "### Add your API Keys\n",
        "We'll need a wandb api key and an exa api key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97aa36f0",
      "metadata": {
        "id": "97aa36f0"
      },
      "outputs": [],
      "source": [
        "# from dotenv import load_dotenv, find_dotenv\n",
        "# load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fe67f23a",
      "metadata": {
        "id": "fe67f23a"
      },
      "outputs": [],
      "source": [
        "# or\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = \"5229a3f594a0e84d921b07bdacc394c913963e2d\"\n",
        "os.environ[\"EXA_API_KEY\"] = \"a99a1436-71f8-4133-a16f-0207a29d9c31\"\n",
        "\n",
        "# or colab secrets:\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\n",
        "# os.environ[\"EXA_API_KEY\"] = userdata.get(\"EXA_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78cf6667",
      "metadata": {
        "id": "78cf6667"
      },
      "source": [
        "## Model Settings\n",
        "Define a model to use, as we are going to use tool calling you need a capable model like `Kimi-K2` or `GLM`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8874a614",
      "metadata": {
        "id": "8874a614"
      },
      "outputs": [],
      "source": [
        "class Console(RichConsole):\n",
        "    def md(self, text):\n",
        "        return self.print(Markdown(text))\n",
        "\n",
        "console = Console()\n",
        "\n",
        "MODEL_SMALL = \"Qwen/Qwen3-235B-A22B-Instruct-2507\"\n",
        "MODEL_MEDIUM = \"zai-org/GLM-4.5\"\n",
        "MODEL_LARGE = \"moonshotai/Kimi-K2-Instruct\"\n",
        "\n",
        "WANDB_ENTITY = \"team-santmak\"\n",
        "WANDB_PROJECT = \"london-workshop-2025\"\n",
        "\n",
        "oai_client = openai.OpenAI(\n",
        "    base_url='https://api.inference.wandb.ai/v1',\n",
        "    api_key=os.getenv(\"WANDB_API_KEY\"),\n",
        "    project=f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")\n",
        "exa_client = Exa(api_key=os.getenv(\"EXA_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d48c55be",
      "metadata": {
        "id": "d48c55be"
      },
      "source": [
        "Let's log to [W&B Weave](https://weave-docs.wandb.ai/). Weights & Biases (W&B) Weave is a framework for tracking, experimenting with, evaluating, deploying, and improving LLM-based applications. Designed for flexibility and scalability, Weave supports every stage of your LLM application development workflow:\n",
        "\n",
        "- Tracing & Monitoring: Track LLM calls and application logic to debug and analyze production systems.\n",
        "- Systematic Iteration: Refine and iterate on prompts, datasets, and models.\n",
        "- Experimentation: Experiment with different models and prompts in the LLM Playground.\n",
        "- Evaluation: Use custom or pre-built scorers alongside our comparison tools to systematically assess and enhance application performance.\n",
        "- Guardrails: Protect your application with pre- and post-safeguards for content moderation, prompt safety, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e70b57d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e70b57d9",
        "outputId": "6f7f04f1-4391-4195-e675-e0331ac986d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1mweave\u001b[0m: wandb version 0.22.3 is available!  To upgrade, please run:\n",
            "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: santiago-makoszay.\n",
            "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/team-santmak/london-workshop-2025/weave\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<weave.trace.weave_client.WeaveClient at 0x7b3e0a30ed50>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "# Initialize a Weave project. Traces will be sent here.\n",
        "# You can view them in the Weave UI (usually runs locally).\n",
        "weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13266e05",
      "metadata": {
        "id": "13266e05"
      },
      "source": [
        "## 1. Basic LLM Call with OpenAI SDK\n",
        "\n",
        "Let's start with a simple call to the LLM using/\n",
        "\n",
        "![](https://github.com/morganmcg1/deep-research-bot/blob/main/notebooks/images/01_trace.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f9ec1b77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "87f31f0f80d34f0e80edb32303fc9f7f",
            "8c62c91856aa4940b595d85c9fca758a"
          ]
        },
        "collapsed": true,
        "id": "f9ec1b77",
        "outputId": "a47d372e-2e3e-4894-94c4-7fc5ca532bd1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87f31f0f80d34f0e80edb32303fc9f7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1mweave\u001b[0m: wandb version 0.22.3 is available!  To upgrade, please run:\n",
            "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: santiago-makoszay.\n",
            "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/team-santmak/london-workshop-2025/weave\n",
            "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/team-santmak/london-workshop-2025/r/call/019a4ed3-a198-744b-8d16-7f3da47f5683\n"
          ]
        }
      ],
      "source": [
        "## Define a simple message list (conversation history)\n",
        "messages = [{\"role\": \"user\", \"content\": \"Hello, LLM! How does an AI agent work?\"}]\n",
        "\n",
        "# Make the callOpen In Colab\n",
        "#Introduction to Building LLM Agents with Tools and Tracing\n",
        "#This script walks through the process of building a simple LLM-powered agent that can use tools (functions) to answer questions. We'll cover:\n",
        "\n",
        "#Making basic LLM calls.\n",
        "#Introducing Weave for tracing and observability.\n",
        "#Defining tools for the LLM (manually and automatically).\n",
        "#Implementing a basic agentic loop.\n",
        "#Structuring the agent using Python classes.\n",
        "#Running the agent on a multi-step task.\n",
        "#Prerequisites: Make sure you have the necessary libraries installed:\n",
        "\n",
        "#!uv pip install -qqq git+https://github.com/morganmcg1/deep-research-bot.git\n",
        "\n",
        "[ ]\n",
        "!uv pip install -qqq git+https://github.com/morganmcg1/deep-research-bot.git\n",
        "\n",
        "[ ]\n",
        "# Global Configuration & Setup\n",
        "import os\n",
        "import inspect\n",
        "import json\n",
        "import weave # Must import weave before litellm for auto-patching\n",
        "import openai\n",
        "from enum import Enum\n",
        "from pydantic import BaseModel, Field\n",
        "from rich.panel import Panel\n",
        "from rich.markdown import Markdown\n",
        "from rich.console import Console as RichConsole\n",
        "from exa_py import Exa\n",
        "from typing import Any, Callable, get_type_hints\n",
        "#Add your API Keys\n",
        "#We'll need a wandb api key and an exa api key\n",
        "\n",
        "\n",
        "[ ]\n",
        "# from dotenv import load_dotenv, find_dotenv\n",
        "# load_dotenv(find_dotenv())\n",
        "\n",
        "[ ]\n",
        "# or\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = \"5229a3f594a0e84d921b07bdacc394c913963e2d\"\n",
        "os.environ[\"EXA_API_KEY\"] = \"a99a1436-71f8-4133-a16f-0207a29d9c31\"\n",
        "\n",
        "# or colab secrets:\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\n",
        "# os.environ[\"EXA_API_KEY\"] = userdata.get(\"EXA_API_KEY\")\n",
        "\n",
        "#Model Settings\n",
        "#Define a model to use, as we are going to use tool calling you need a capable model like Kimi-K2 or GLM\n",
        "\n",
        "\n",
        "[ ]\n",
        "class Console(RichConsole):\n",
        "    def md(self, text):\n",
        "        return self.print(Markdown(text))\n",
        "\n",
        "console = Console()\n",
        "\n",
        "MODEL_SMALL = \"Qwen/Qwen3-235B-A22B-Instruct-2507\"\n",
        "MODEL_MEDIUM = \"zai-org/GLM-4.5\"\n",
        "MODEL_LARGE = \"moonshotai/Kimi-K2-Instruct\"\n",
        "\n",
        "\n",
        "#Let's log to W&B Weave. Weights & Biases (W&B) Weave is a framework for tracking, experimenting with, evaluating, deploying, and improving LLM-based applications. Designed for flexibility and scalability, Weave supports every stage of your LLM application development workflow:\n",
        "\n",
        "#Tracing & Monitoring: Track LLM calls and application logic to debug and analyze production systems.\n",
        "#Systematic Iteration: Refine and iterate on prompts, datasets, and models.\n",
        "#Experimentation: Experiment with different models and prompts in the LLM Playground.\n",
        "#Evaluation: Use custom or pre-built scorers alongside our comparison tools to systematically assess and enhance application performance.\n",
        "#Guardrails: Protect your application with pre- and post-safeguards for content moderation, prompt safety, and more.\n",
        "\n",
        "[ ]\n",
        "\n",
        "# Initialize a Weave project. Traces will be sent here.\n",
        "# You can view them in the Weave UI (usually runs locally).\n",
        "weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")\n",
        "#1. Basic LLM Call with OpenAI SDK\n",
        "#Let's start with a simple call to the LLM using/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[ ]\n",
        "# Define a simple message list (conversation history)\n",
        "messages = [{\"role\": \"user\", \"content\": \"Hello, LLM! How does an AI agent work?\"}]\n",
        "\n",
        "# Make the call\n",
        "response = resp = oai_client.chat.completions.create(\n",
        "    model = MODEL_SMALL,\n",
        "    messages=messages,\n",
        "    max_tokens=400,\n",
        ")\n",
        "# Print the response content\n",
        "\n",
        "#Because we imported weave and called weave.init(), the OpenAI SDK call above was automatically traced. You can open your Weave dashboard and see the trace, including the input messages, output response, latency, model used, etc. This is invaluable for debugging and monitoring.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "795ead84",
      "metadata": {
        "id": "795ead84"
      },
      "source": [
        "Because we imported `weave` and called `weave.init()`, the OpenAI SDK call above was automatically traced. You can open your Weave dashboard and see the trace, including the input messages, output response, latency, model used, etc. This is invaluable for debugging and monitoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "eb90b937",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "eb90b937",
        "outputId": "c5a7e2fa-b508-4e2b-eff4-799630d23e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/team-santmak/london-workshop-2025/r/call/019a4ed3-f292-73d1-b749-14763a0c5469\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Hello! Great question! ğŸ˜Š An \u001b[1mAI agent\u001b[0m is a software system that perceives its environment, makes decisions, and    \n",
              "takes actions to achieve specific goals. Think of it like a digital \"robot\" that can think and act autonomouslyâ€”or \n",
              "semi-autonomouslyâ€”based on input and objectives.                                                                   \n",
              "\n",
              "Hereâ€™s a breakdown of how an AI agent works:                                                                       \n",
              "\n",
              "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
              "                                                 \u001b[1mğŸ” 1. \u001b[0m\u001b[1mPerception\u001b[0m                                                  \n",
              "\n",
              "The agent gathers information from its environment using sensors or data inputs. This could include:               \n",
              "\n",
              "\u001b[1;33m â€¢ \u001b[0mText input (like your message to me)                                                                            \n",
              "\u001b[1;33m â€¢ \u001b[0mSensor data (in robotics: cameras, microphones, GPS)                                                            \n",
              "\u001b[1;33m â€¢ \u001b[0mDatabase queries                                                                                                \n",
              "\u001b[1;33m â€¢ \u001b[0mWeb APIs                                                                                                        \n",
              "\n",
              "â¡ï¸ Example: A chatbot reads your message. A self-driving car sees traffic signs via cameras.                        \n",
              "\n",
              "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
              "                                         \u001b[1mğŸ§  2. \u001b[0m\u001b[1mDecision-Making (Reasoning)\u001b[0m                                         \n",
              "\n",
              "Using the input, the agent processes the information to decide what to do. This is where AI models (like neural    \n",
              "networks or rule-based systems) come in.                                                                           \n",
              "\n",
              "Key components:                                                                                                    \n",
              "\n",
              "\u001b[1;33m â€¢ \u001b[0m\u001b[1mKnowledge base\u001b[0m: What the agent \"knows\"                                                                          \n",
              "\u001b[1;33m â€¢ \u001b[0m\u001b[1mGoals\u001b[0m: What it's trying to achieve                                                                              \n",
              "\u001b[1;33m â€¢ \u001b[0m\u001b[1mModel of the world\u001b[0m: How it understands cause and effect                                                         \n",
              "\u001b[1;33m â€¢ \u001b[0m\u001b[1mAlgorithms\u001b[0m: Such as machine learning, search, logic, or planning                                                \n",
              "\n",
              "â¡ï¸ Example: A recommendation agent analyzes your past behavior to decide what product to suggest.                   \n",
              "\n",
              "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
              "                                                   \u001b[1mğŸ¤– 3. \u001b[0m\u001b[1mAction\u001b[0m                                                    \n",
              "\n",
              "The agent performs an action based on its decision. This could be:                                                 \n",
              "\n",
              "\u001b[1;33m â€¢ \u001b[0mSending a message (like this one!)                                                                              \n",
              "\u001b[1;33m â€¢ \u001b[0mMoving a robot arm                                                                                              \n",
              "\u001b[1;33m â€¢ \u001b[0mAdjusting thermostat settings                                                                                   \n",
              "\u001b[1;33m â€¢ \u001b[0mPlacing a trade in a stock market                                                                               \n",
              "\n",
              "â¡ï¸ Example: A virtual assistant says, â€œTurning on the lights.â€                                                      \n",
              "\n",
              "\u001b[33mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
              "                                    \u001b[1mğŸ” 4. \u001b[0m\u001b[1mFeedback Loop (Learning & Adaptation)\u001b[0m                                    \n",
              "\n",
              "Many AI agents learn from experience. They use feedback (like rewards or errors) to improve over time.             \n",
              "\n",
              "Types of learning:                                                                                                 \n",
              "\n",
              "\u001b[1;33m â€¢ \u001b[0m\u001b[1mReinforcement Learning\u001b[0m: Learn by trial                                                                          \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hello! Great question! ğŸ˜Š An <span style=\"font-weight: bold\">AI agent</span> is a software system that perceives its environment, makes decisions, and    \n",
              "takes actions to achieve specific goals. Think of it like a digital \"robot\" that can think and act autonomouslyâ€”or \n",
              "semi-autonomouslyâ€”based on input and objectives.                                                                   \n",
              "\n",
              "Hereâ€™s a breakdown of how an AI agent works:                                                                       \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
              "                                                 <span style=\"font-weight: bold\">ğŸ” 1. Perception</span>                                                  \n",
              "\n",
              "The agent gathers information from its environment using sensors or data inputs. This could include:               \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Text input (like your message to me)                                                                            \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Sensor data (in robotics: cameras, microphones, GPS)                                                            \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Database queries                                                                                                \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Web APIs                                                                                                        \n",
              "\n",
              "â¡ï¸ Example: A chatbot reads your message. A self-driving car sees traffic signs via cameras.                        \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
              "                                         <span style=\"font-weight: bold\">ğŸ§  2. Decision-Making (Reasoning)</span>                                         \n",
              "\n",
              "Using the input, the agent processes the information to decide what to do. This is where AI models (like neural    \n",
              "networks or rule-based systems) come in.                                                                           \n",
              "\n",
              "Key components:                                                                                                    \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">Knowledge base</span>: What the agent \"knows\"                                                                          \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">Goals</span>: What it's trying to achieve                                                                              \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">Model of the world</span>: How it understands cause and effect                                                         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">Algorithms</span>: Such as machine learning, search, logic, or planning                                                \n",
              "\n",
              "â¡ï¸ Example: A recommendation agent analyzes your past behavior to decide what product to suggest.                   \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
              "                                                   <span style=\"font-weight: bold\">ğŸ¤– 3. Action</span>                                                    \n",
              "\n",
              "The agent performs an action based on its decision. This could be:                                                 \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Sending a message (like this one!)                                                                              \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Moving a robot arm                                                                                              \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Adjusting thermostat settings                                                                                   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span>Placing a trade in a stock market                                                                               \n",
              "\n",
              "â¡ï¸ Example: A virtual assistant says, â€œTurning on the lights.â€                                                      \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
              "                                    <span style=\"font-weight: bold\">ğŸ” 4. Feedback Loop (Learning &amp; Adaptation)</span>                                    \n",
              "\n",
              "Many AI agents learn from experience. They use feedback (like rewards or errors) to improve over time.             \n",
              "\n",
              "Types of learning:                                                                                                 \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> â€¢ </span><span style=\"font-weight: bold\">Reinforcement Learning</span>: Learn by trial                                                                          \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# most of the time you would want to define your own operations to trace, for instance to call the model.\n",
        "# You just need to add the @weave.op decorator to the function and it will be traced.\n",
        "\n",
        "@weave.op\n",
        "def call_model(model_name: str, messages: list[dict[str, Any]], **kwargs) -> str:\n",
        "    \"Call a model with the given messages and kwargs.\"\n",
        "    response = oai_client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=messages,\n",
        "        max_tokens=400,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message\n",
        "\n",
        "response = call_model(model_name=MODEL_SMALL, messages=messages)\n",
        "console.md(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e97c8b6",
      "metadata": {
        "id": "4e97c8b6"
      },
      "source": [
        "![](https://github.com/morganmcg1/deep-research-bot/blob/main/notebooks/images/02_nested_trace.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75409570",
      "metadata": {
        "id": "75409570"
      },
      "source": [
        "\n",
        "## 2. Introducing Tool Calling\n",
        "\n",
        "Agents become much more powerful when they can use **tools** â€“ external functions or APIs â€“ to get information or perform actions beyond the LLM's internal knowledge. To allow an LLM to use a tool, we need to provide it with a description (schema) of the tool, including its name, purpose, and expected arguments.\n",
        "\n",
        "Check the Mistral docs for function calling: https://platform.openai.com/docs/guides/function-calling\n",
        "\n",
        "![](https://github.com/morganmcg1/deep-research-bot/blob/main/notebooks/images/function-calling-diagram-steps.png?raw=1)\n",
        "\n",
        "First, let's define a simple Python function we want the LLM to be able to call. We add `@weave.op` to trace when this function actually gets executed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2ede106b",
      "metadata": {
        "id": "2ede106b"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "def add_numbers(a: int, b: int) -> int:\n",
        "    \"\"\"Use this tool to add numbers.\n",
        "    Args:\n",
        "        a: The first number.\n",
        "        b: The second number.\n",
        "    \"\"\"\n",
        "    return a + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a691123a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a691123a",
        "outputId": "3db11191-d6f9-411b-dcfe-3e0b7d76b8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/team-santmak/london-workshop-2025/r/call/019a4ed4-e85d-70d5-a5c8-bc92276a37e4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "add_numbers(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ffcbfd9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "collapsed": true,
        "id": "ffcbfd9a",
        "outputId": "1653ba9e-6f0b-4036-9442-87318ec5104a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/team-santmak/london-workshop-2025/r/call/019a4ed4-fa89-7544-83a8-7eaa44341713\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type function is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-74899773.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this doesn't work...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcall_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_SMALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madd_numbers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1251\u001b[0m                 \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                     res, _ = _call_sync_func(\n\u001b[0m\u001b[1;32m   1254\u001b[0m                         \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__should_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m_call_sync_func\u001b[0;34m(op, __weave, __should_raise, __require_explicit_finish, *args, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1456281471.py\u001b[0m in \u001b[0;36mcall_model\u001b[0;34m(model_name, messages, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcall_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"Call a model with the given messages and kwargs.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     response = oai_client.chat.completions.create(\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1251\u001b[0m                 \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                     res, _ = _call_sync_func(\n\u001b[0m\u001b[1;32m   1254\u001b[0m                         \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__should_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/weave/trace/op.py\u001b[0m in \u001b[0;36m_call_sync_func\u001b[0;34m(op, __weave, __should_raise, __require_explicit_finish, *args, **kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/weave/integrations/openai/openai_sdk.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m                         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream_options\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"include_usage\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1155\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1157\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0mremaining_retries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_retries\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mretries_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries_taken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretries_taken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_build_request\u001b[0;34m(self, options, retries_taken)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;31m# TODO: report this error to httpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         return self._client.build_request(  # pyright: ignore[reportUnknownMemberType]\n\u001b[0m\u001b[1;32m    548\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotGiven\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mbuild_request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, timeout, extensions)\u001b[0m\n\u001b[1;32m    376\u001b[0m             )\n\u001b[1;32m    377\u001b[0m             \u001b[0mextensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         return Request(\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, method, url, params, headers, cookies, content, data, files, json, stream, extensions)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mcontent_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content-type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             headers, stream = encode_request(\n\u001b[0m\u001b[1;32m    409\u001b[0m                 \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_content.py\u001b[0m in \u001b[0;36mencode_request\u001b[0;34m(content, data, files, json, boundary)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencode_urlencoded_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mjson\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mencode_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mByteStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_content.py\u001b[0m in \u001b[0;36mencode_json\u001b[0;34m(json)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mByteStream\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     body = json_dumps(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     ).encode(\"utf-8\")\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type function is not JSON serializable"
          ]
        }
      ],
      "source": [
        "# this doesn't work...\n",
        "call_model(model_name=MODEL_SMALL, messages=messages, tools=[add_numbers])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b082a58a",
      "metadata": {
        "id": "b082a58a"
      },
      "source": [
        "> We need to manually create the JSON schema describing this tool in a format that models *Mistral* understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "190a9063",
      "metadata": {
        "id": "190a9063"
      },
      "outputs": [],
      "source": [
        "# Manually define the tool schema\n",
        "tool_add_numbers_schema = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"add_numbers\",\n",
        "        \"description\": \"Adds two numbers.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"a\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"The first number.\"\n",
        "                },\n",
        "                \"b\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"The second number.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"a\", \"b\"]\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b6dfd4",
      "metadata": {
        "id": "f8b6dfd4"
      },
      "source": [
        "Now, we make an LLM call, passing the `tools` parameter with our schema. We ask a question that should trigger the tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0e7d2900",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "0e7d2900",
        "outputId": "c7c282d9-1413-4fbf-e48c-6e96e0776a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/team-santmak/london-workshop-2025/r/call/019a4ed5-5b6f-7ad0-93b8-4d8afae4553d\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mcontent\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mrefusal\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
              "    \u001b[33mannotations\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33maudio\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\n",
              "        \u001b[1;35mChatCompletionMessageFunctionToolCall\u001b[0m\u001b[1m(\u001b[0m\n",
              "            \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-tool-dffaa4f384f844119b02f1256348e41f'\u001b[0m,\n",
              "            \u001b[33mfunction\u001b[0m=\u001b[1;35mFunction\u001b[0m\u001b[1m(\u001b[0m\u001b[33marguments\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"a\": 77, \"b\": 11\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m, \u001b[33mname\u001b[0m=\u001b[32m'add_numbers'\u001b[0m\u001b[1m)\u001b[0m,\n",
              "            \u001b[33mtype\u001b[0m=\u001b[32m'function'\u001b[0m\n",
              "        \u001b[1m)\u001b[0m\n",
              "    \u001b[1m]\u001b[0m,\n",
              "    \u001b[33mreasoning_content\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">refusal</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">annotations</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">audio</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessageFunctionToolCall</span><span style=\"font-weight: bold\">(</span>\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-tool-dffaa4f384f844119b02f1256348e41f'</span>,\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">function</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Function</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">arguments</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{\"a\": 77, \"b\": 11}'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'add_numbers'</span><span style=\"font-weight: bold\">)</span>,\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>\n",
              "        <span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">reasoning_content</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant use tools to answer questions.\"},\n",
        "    {\"role\": \"user\", \"content\": \"My lucky numbers are 77 and 11. What is their sum?\"}]\n",
        "response = call_model(model_name=MODEL_SMALL, messages=messages, tools=[tool_add_numbers_schema])\n",
        "console.print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "092fdded",
      "metadata": {
        "id": "092fdded"
      },
      "source": [
        "## Manual Tool Call\n",
        "The LLM's response might contain a request to call our tool (`response.choices[0].message.tool_calls`) or it might respond directly (`response.choices[0].message.content`). If it requests a tool call, we need to:\n",
        "\n",
        "1. Parse the arguments it provides.\n",
        "2. Execute our actual Python function (`add_numbers`) with those arguments.\n",
        "3. (In a real agent loop) Send the result back to the LLM in a new message with `role=\"tool\"`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10f1ea6c",
      "metadata": {
        "id": "10f1ea6c"
      },
      "source": [
        "Let's manually call the tools in the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b8e64e83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "id": "b8e64e83",
        "outputId": "c35cb639-a0ac-48c9-ace6-30ab7d99b9dc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "LLM requested a tool call:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM requested a tool call:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  - Tool: add_numbers, Args: \u001b[1m{\u001b[0m\u001b[32m\"a\"\u001b[0m: \u001b[1;36m77\u001b[0m, \u001b[32m\"b\"\u001b[0m: \u001b[1;36m11\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  - Tool: add_numbers, Args: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"a\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"b\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/team-santmak/london-workshop-2025/r/call/019a4ed5-96a7-7e26-9d85-32fa8b475e44\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Final Result: \u001b[1;36m88\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Final Result: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "if response.tool_calls:\n",
        "    console.print(\"LLM requested a tool call:\")\n",
        "    for tool_call in response.tool_calls:\n",
        "        function_name = tool_call.function.name\n",
        "        function_args_str = tool_call.function.arguments\n",
        "        function_args = json.loads(function_args_str)\n",
        "        console.print(f\"  - Tool: {function_name}, Args: {function_args_str}\")\n",
        "        if function_name == \"add_numbers\":\n",
        "            tool_result_content = add_numbers(**function_args)\n",
        "\n",
        "console.print(f\"Final Result: {tool_result_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8f38a9f",
      "metadata": {
        "id": "e8f38a9f"
      },
      "source": [
        "We need to add the tool call result to the messages (there is actually 2 messages to add)\n",
        "- the response from the assistant that decided to call the tool\n",
        "- the tool output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dd2ca3b8",
      "metadata": {
        "id": "dd2ca3b8"
      },
      "outputs": [],
      "source": [
        "messages.append(response.model_dump())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d13d8f5a",
      "metadata": {
        "id": "d13d8f5a"
      },
      "outputs": [],
      "source": [
        "messages.append({\n",
        "    \"tool_call_id\": tool_call.id,\n",
        "    \"role\": \"tool\",\n",
        "    \"name\": function_name,\n",
        "    \"content\": str(tool_result_content)\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a0992e3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0992e3a",
        "outputId": "0106645a-fa29-4e36-ed57-99264fa128b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': 'You are a helpful assistant use tools to answer questions.'},\n",
              " {'role': 'user',\n",
              "  'content': 'My lucky numbers are 77 and 11. What is their sum?'},\n",
              " {'content': None,\n",
              "  'refusal': None,\n",
              "  'role': 'assistant',\n",
              "  'annotations': None,\n",
              "  'audio': None,\n",
              "  'function_call': None,\n",
              "  'tool_calls': [{'id': 'chatcmpl-tool-dffaa4f384f844119b02f1256348e41f',\n",
              "    'function': {'arguments': '{\"a\": 77, \"b\": 11}', 'name': 'add_numbers'},\n",
              "    'type': 'function'}],\n",
              "  'reasoning_content': None},\n",
              " {'tool_call_id': 'chatcmpl-tool-dffaa4f384f844119b02f1256348e41f',\n",
              "  'role': 'tool',\n",
              "  'name': 'add_numbers',\n",
              "  'content': '88'}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f98c76bf",
      "metadata": {
        "id": "f98c76bf"
      },
      "source": [
        "You should have a sequence of messages like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a5621c18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5621c18",
        "outputId": "917cc49c-c313-4d68-90b7-c96f83e05fba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['system', 'user', 'assistant', 'tool']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "[m[\"role\"] for m in messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50a4b05f",
      "metadata": {
        "id": "50a4b05f"
      },
      "source": [
        "Now call the model again with the new messages and it will use the tool call result to answer the question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "fbf845aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "fbf845aa",
        "outputId": "fbe3e4d8-cfa3-43f6-c629-2b7f2c35c639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/team-santmak/london-workshop-2025/r/call/019a4ed5-dd25-7879-9c17-b8afa87b3547\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "The sum of your lucky numbers, \u001b[1;36m77\u001b[0m and \u001b[1;36m11\u001b[0m, is **\u001b[1;36m88\u001b[0m**.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The sum of your lucky numbers, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77</span> and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, is **<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88</span>**.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "final_response = call_model(model_name=MODEL_SMALL, messages=messages)\n",
        "console.print(final_response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e01abc5",
      "metadata": {
        "id": "0e01abc5"
      },
      "source": [
        "## 3. Simplifying Tool Definition with a Processor Function\n",
        "\n",
        "Manually writing JSON schemas is tedious and error-prone. We can automate this by inspecting our Python function's signature, type hints, and docstring.\n",
        "\n",
        "First, let's define a helper function (`generate_tool_schema`) that takes a Python function and generates the schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "61ab93b4",
      "metadata": {
        "id": "61ab93b4"
      },
      "outputs": [],
      "source": [
        "def generate_tool_schema(func: Callable) -> dict:\n",
        "    \"\"\"Given a Python function, generate a tool-compatible JSON schema.\n",
        "    Handles basic types and Enums. Assumes docstrings are formatted for arg descriptions.\n",
        "    \"\"\"\n",
        "    signature = inspect.signature(func)\n",
        "    parameters = signature.parameters\n",
        "    type_hints = get_type_hints(func)\n",
        "\n",
        "    schema = {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": func.__name__,\n",
        "            \"description\": inspect.getdoc(func).split(\"\\\\n\")[0] if inspect.getdoc(func) else \"\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {},\n",
        "                \"required\": [],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "    docstring = inspect.getdoc(func)\n",
        "    param_descriptions = {}\n",
        "    if docstring:\n",
        "        args_section = False\n",
        "        current_param = None\n",
        "        for line in docstring.split('\\\\n'):\n",
        "            line_stripped = line.strip()\n",
        "            if line_stripped.lower().startswith((\"args:\", \"arguments:\", \"parameters:\")):\n",
        "                args_section = True\n",
        "                continue\n",
        "            if args_section:\n",
        "                if \":\" in line_stripped:\n",
        "                    param_name, desc = line_stripped.split(\":\", 1)\n",
        "                    param_descriptions[param_name.strip()] = desc.strip()\n",
        "                elif line_stripped and not line_stripped.startswith(\" \"): # Heuristic: end of args section\n",
        "                     args_section = False\n",
        "\n",
        "    for name, param in parameters.items():\n",
        "        is_required = param.default == inspect.Parameter.empty\n",
        "        param_type = type_hints.get(name, Any)\n",
        "        json_type = \"string\"\n",
        "        param_schema = {}\n",
        "\n",
        "        # Basic type mapping\n",
        "        if param_type == str: json_type = \"string\"\n",
        "        elif param_type == int: json_type = \"integer\"\n",
        "        elif param_type == float: json_type = \"number\"\n",
        "        elif param_type == bool: json_type = \"boolean\"\n",
        "        elif hasattr(param_type, '__origin__') and param_type.__origin__ is list: # Handle list[type]\n",
        "             item_type = param_type.__args__[0] if param_type.__args__ else Any\n",
        "             if item_type == str: param_schema = {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
        "             elif item_type == int: param_schema = {\"type\": \"array\", \"items\": {\"type\": \"integer\"}}\n",
        "             # Add more list item types if needed\n",
        "             else: param_schema = {\"type\": \"array\", \"items\": {\"type\": \"string\"}} # Default list item type\n",
        "        elif hasattr(param_type, \"__members__\") and issubclass(param_type, Enum): # Handle Enum\n",
        "             json_type = \"string\"\n",
        "             param_schema[\"enum\"] = [e.value for e in param_type]\n",
        "\n",
        "        if not param_schema: # If not set by list or Enum\n",
        "            param_schema[\"type\"] = json_type\n",
        "\n",
        "        param_schema[\"description\"] = param_descriptions.get(name, \"\")\n",
        "\n",
        "        if param.default != inspect.Parameter.empty and param.default is not None:\n",
        "             param_schema[\"default\"] = param.default # Note: OpenAI schema doesn't officially use default, but useful metadata\n",
        "\n",
        "        schema[\"function\"][\"parameters\"][\"properties\"][name] = param_schema\n",
        "        if is_required:\n",
        "            schema[\"function\"][\"parameters\"][\"required\"].append(name)\n",
        "    return schema"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d193839",
      "metadata": {
        "id": "8d193839"
      },
      "source": [
        "Now we can use this function to automatically generate the schema for our tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "187d550c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "187d550c",
        "outputId": "604694c2-350a-405d-e370-a8782644e4d2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'type'\u001b[0m: \u001b[32m'function'\u001b[0m,\n",
              "    \u001b[32m'function'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[32m'name'\u001b[0m: \u001b[32m'add_numbers'\u001b[0m,\n",
              "        \u001b[32m'description'\u001b[0m: \u001b[32m'Use this tool to add numbers.\\nArgs:\\n    a: The first number.\\n    b: The second number.'\u001b[0m,\n",
              "        \u001b[32m'parameters'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'type'\u001b[0m: \u001b[32m'object'\u001b[0m,\n",
              "            \u001b[32m'properties'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "                \u001b[32m'a'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'integer'\u001b[0m, \u001b[32m'description'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
              "                \u001b[32m'b'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'integer'\u001b[0m, \u001b[32m'description'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m\n",
              "            \u001b[1m}\u001b[0m,\n",
              "            \u001b[32m'required'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'a'\u001b[0m, \u001b[32m'b'\u001b[0m\u001b[1m]\u001b[0m\n",
              "        \u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'add_numbers'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Use this tool to add numbers.\\nArgs:\\n    a: The first number.\\n    b: The second number.'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'parameters'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'object'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'properties'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "                <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'integer'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
              "                <span style=\"color: #008000; text-decoration-color: #008000\">'b'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'integer'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>\n",
              "            <span style=\"font-weight: bold\">}</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'required'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'b'</span><span style=\"font-weight: bold\">]</span>\n",
              "        <span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "tool_schema = generate_tool_schema(add_numbers)\n",
        "console.print(tool_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7867416e",
      "metadata": {
        "id": "7867416e"
      },
      "source": [
        "Now, we define a `function_tool` \"processor\". This isn't a decorator in the `@` syntax sense here, but a function that we call *after* defining our tool function. It uses `generate_tool_schema` to attach the schema to the function object itself.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "015ed9a3",
      "metadata": {
        "id": "015ed9a3"
      },
      "outputs": [],
      "source": [
        "def function_tool(func: Callable) -> Callable:\n",
        "    \"\"\"Attaches a tool schema to the function and marks it as a tool.\n",
        "    Call this *after* defining your function: my_func = function_tool(my_func)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        func.tool_schema = generate_tool_schema(func)\n",
        "        func.is_tool = True # Mark it as a tool\n",
        "    except Exception as e:\n",
        "        console.print(f\"Error processing tool {func.__name__}: {e}\")\n",
        "        # Optionally raise or mark as failed\n",
        "        func.tool_schema = None\n",
        "        func.is_tool = False\n",
        "    return func"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "504243c6",
      "metadata": {
        "id": "504243c6"
      },
      "source": [
        "We can use this function to automatically generate the schema for our tool, as a decorator or after the function is defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8c424f21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "8c424f21",
        "outputId": "53eeaed3-5d38-4d9c-a46d-07990e1436a7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'type'\u001b[0m: \u001b[32m'function'\u001b[0m,\n",
              "    \u001b[32m'function'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "        \u001b[32m'name'\u001b[0m: \u001b[32m'add_numbers'\u001b[0m,\n",
              "        \u001b[32m'description'\u001b[0m: \u001b[32m'Use this tool to add numbers.\\nArgs:\\n    a: The first number.\\n    b: The second number.'\u001b[0m,\n",
              "        \u001b[32m'parameters'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "            \u001b[32m'type'\u001b[0m: \u001b[32m'object'\u001b[0m,\n",
              "            \u001b[32m'properties'\u001b[0m: \u001b[1m{\u001b[0m\n",
              "                \u001b[32m'a'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'integer'\u001b[0m, \u001b[32m'description'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m,\n",
              "                \u001b[32m'b'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'integer'\u001b[0m, \u001b[32m'description'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m\n",
              "            \u001b[1m}\u001b[0m,\n",
              "            \u001b[32m'required'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'a'\u001b[0m, \u001b[32m'b'\u001b[0m\u001b[1m]\u001b[0m\n",
              "        \u001b[1m}\u001b[0m\n",
              "    \u001b[1m}\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'function'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'add_numbers'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Use this tool to add numbers.\\nArgs:\\n    a: The first number.\\n    b: The second number.'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'parameters'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'object'</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'properties'</span>: <span style=\"font-weight: bold\">{</span>\n",
              "                <span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'integer'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>,\n",
              "                <span style=\"color: #008000; text-decoration-color: #008000\">'b'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'integer'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>\n",
              "            <span style=\"font-weight: bold\">}</span>,\n",
              "            <span style=\"color: #008000; text-decoration-color: #008000\">'required'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'b'</span><span style=\"font-weight: bold\">]</span>\n",
              "        <span style=\"font-weight: bold\">}</span>\n",
              "    <span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "add_numbers = function_tool(add_numbers)\n",
        "console.print(add_numbers.tool_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "8c651811",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c651811",
        "outputId": "0692e2e3-26e7-4074-e25e-9f6d8612531f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'function',\n",
              " 'function': {'name': 'add_numbers',\n",
              "  'description': 'Use this tool to add numbers.\\nArgs:\\n    a: The first number.\\n    b: The second number.',\n",
              "  'parameters': {'type': 'object',\n",
              "   'properties': {'a': {'type': 'integer', 'description': ''},\n",
              "    'b': {'type': 'integer', 'description': ''}},\n",
              "   'required': ['a', 'b']}}}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "add_numbers.tool_schema"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7280921",
      "metadata": {
        "id": "d7280921"
      },
      "source": [
        "and call the tool =)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "70bd946a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70bd946a",
        "outputId": "a25a287c-9989-4dc2-9f64-8a875b2e1219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m\u001b[1mweave\u001b[0m: ğŸ© https://wandb.ai/team-santmak/london-workshop-2025/r/call/019a4ed6-6c32-7f30-b56e-4d9de70d6321\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "add_numbers(1, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "265ef16e",
      "metadata": {
        "id": "265ef16e"
      },
      "source": [
        "### 3.1 Real Example using an API based tool"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5227a8a9",
      "metadata": {
        "id": "5227a8a9"
      },
      "source": [
        "We are going to use the [EXA search API](https://docs.exa.ai/reference/getting-started).\n",
        "- How does [EXA search works](https://docs.exa.ai/reference/how-exa-search-works#how-exa-search-works)\n",
        "- Using exa search [as tool calling](https://docs.exa.ai/reference/openai-tool-calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0335e62",
      "metadata": {
        "id": "a0335e62"
      },
      "outputs": [],
      "source": [
        "query = \"Recipes for cooking seabass?\"\n",
        "\n",
        "search_res = exa_client.search_and_contents(query=query, type='auto', num_results=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e162a4a9",
      "metadata": {
        "id": "e162a4a9"
      },
      "outputs": [],
      "source": [
        "console.print(search_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e67727",
      "metadata": {
        "id": "19e67727"
      },
      "source": [
        "Let's explore the payload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea673071",
      "metadata": {
        "id": "ea673071"
      },
      "outputs": [],
      "source": [
        "console.md(\"\\n-------------------\\n\".join(result.text for result in search_res.results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7972869e",
      "metadata": {
        "id": "7972869e"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "@function_tool # <- we can use the decorator to automatically generate the tool schema\n",
        "def exa_search(query: str, num_results: int = 5) -> list[dict[str, str]]:\n",
        "    \"\"\"Perform a search query on the web and retrieve the most relevant URLs and web content.\n",
        "\n",
        "    This function uses the Exa search API to find relevant web pages based on the query\n",
        "    and returns their titles, text content, and URLs.\n",
        "\n",
        "    Args:\n",
        "        query: The search query. Use detailed, specific queries for better results.\n",
        "               The quality of results depends on the specificity of the query.\n",
        "        num_results: The number of search results to retrieve. Defaults to 5.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, each containing:\n",
        "            - title: The title of the web page\n",
        "            - text: The text content of the web page\n",
        "            - url: The URL of the web page\n",
        "    \"\"\"\n",
        "    search_results = exa_client.search_and_contents(query=query, type='auto', num_results=num_results)\n",
        "\n",
        "    output = []\n",
        "    for result in search_results.results:\n",
        "        output.append(\n",
        "            {\"title\": result.title,\n",
        "            \"text\": result.text,\n",
        "            \"url\": result.url\n",
        "            }\n",
        "        )\n",
        "    return output\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1f007ba",
      "metadata": {
        "id": "c1f007ba"
      },
      "outputs": [],
      "source": [
        "exa_search.tool_schema"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3973765",
      "metadata": {
        "id": "f3973765"
      },
      "source": [
        "We get a list of results with the relevant metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb22b679",
      "metadata": {
        "id": "fb22b679"
      },
      "outputs": [],
      "source": [
        "search_results = exa_search(\"How do I cook seabass?\")\n",
        "console.print(search_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a073e8",
      "metadata": {
        "id": "82a073e8"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an agent that has access to an advanced search engine. Please provide the user with the information they are looking for by using the search tool provided. Make sure to keep the sources. Return in markdown format.\"},\n",
        "    {\"role\": \"user\", \"content\": \"How do I cook seabass?\"}]\n",
        "\n",
        "response = call_model(model_name=MODEL_SMALL, messages=messages, tools=[exa_search.tool_schema])\n",
        "console.print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7350625",
      "metadata": {
        "id": "d7350625"
      },
      "source": [
        "Let's create some helper functions to perform the tool calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0938e29b",
      "metadata": {
        "id": "0938e29b"
      },
      "outputs": [],
      "source": [
        "def get_tool(tools: list[Callable], name: str) -> Callable:\n",
        "    for t in tools:\n",
        "        if t.__name__ == name:\n",
        "            return t\n",
        "    raise KeyError(f\"No tool with name {name} found\")\n",
        "\n",
        "def perform_tool_calls(tools: list[Callable], tool_calls: list[Callable]) -> list[dict]:\n",
        "    \"Perform the tool calls and return the messages with the tool call results\"\n",
        "    messages = []\n",
        "    if not tool_calls:\n",
        "        return messages\n",
        "    for tool_call in tool_calls:\n",
        "        function_name = tool_call.function.name\n",
        "        function_args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "        with console.status(f\"[bold cyan]Executing {function_name}...[/bold cyan]\"):\n",
        "            tool = get_tool(tools, function_name)\n",
        "            tool_response = tool(**function_args) # doesn't handle async\n",
        "\n",
        "        # Create panel content\n",
        "        panel_content = f\"[bold cyan]ğŸ”§ Tool Call:[/bold cyan] {function_name}\\n\\n\"\n",
        "        panel_content += f\"[dim]Args: {tool_call.function.arguments}[/dim]\\n\\n\"\n",
        "\n",
        "        if isinstance(tool_response, list):\n",
        "            panel_content += f\"[green]âœ“[/green] Found {len(tool_response)} results\"\n",
        "        else:\n",
        "            panel_content += f\"[green]âœ“[/green] {function_name} executed successfully\"\n",
        "\n",
        "        console.print(Panel(panel_content, border_style=\"cyan\"))\n",
        "\n",
        "        messages.append({\n",
        "            \"tool_call_id\": tool_call.id,\n",
        "            \"role\": \"tool\",\n",
        "            \"content\": str(tool_response),\n",
        "        })\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5881ecb",
      "metadata": {
        "id": "c5881ecb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# add the tool call result to the messages\n",
        "messages.append(response.model_dump())\n",
        "messages.extend(perform_tool_calls(tools=[exa_search], tool_calls=response.tool_calls))\n",
        "messages.append({\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Answer my previous query based on the search results.\",})\n",
        "\n",
        "final_response = call_model(model_name=MODEL_SMALL, messages=messages)\n",
        "console.rule(\"Final Model Response\")\n",
        "console.md(final_response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3956939",
      "metadata": {
        "id": "b3956939"
      },
      "source": [
        "Let's wrap this in a function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10ab1b81",
      "metadata": {
        "id": "10ab1b81"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "def research(query: str) -> str:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an agent that has access to an advanced search engine. Please provide the user with the information they are looking for by using the search tool provided. Make sure to keep the sources. Return in markdown format.\"},\n",
        "        {\"role\": \"user\", \"content\": query}]\n",
        "\n",
        "    # call model with tools\n",
        "    response = call_model(\n",
        "        model_name=MODEL_SMALL,\n",
        "        messages=messages,\n",
        "        tools=[exa_search.tool_schema])\n",
        "\n",
        "    # add the response to the messages\n",
        "    messages.append(response.model_dump())\n",
        "\n",
        "    # perform the tool calls\n",
        "    messages.extend(perform_tool_calls(tools=[exa_search], tool_calls=response.tool_calls))\n",
        "\n",
        "    # prompt the model to be grounded\n",
        "    messages.append({\"role\": \"user\",\"content\": \"Answer my previous query based on the search results.\",})\n",
        "\n",
        "    final_response = call_model(model_name=MODEL_SMALL, messages=messages)\n",
        "    return final_response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc0183b1",
      "metadata": {
        "id": "bc0183b1"
      },
      "outputs": [],
      "source": [
        "result = research(\"What are the most popular pokemons?\")\n",
        "console.md(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dedaa4a4",
      "metadata": {
        "id": "dedaa4a4"
      },
      "source": [
        "![](https://github.com/morganmcg1/deep-research-bot/blob/main/notebooks/images/04_pokedex.png?raw=1)\n",
        "\n",
        "This is \"Almost\" an agent, but it's missing the loop. Let's add that next."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efd890f5",
      "metadata": {
        "id": "efd890f5"
      },
      "source": [
        "## 4. Implementing a Basic Agentic Loop\n",
        "\n",
        "Let's implement a basic agentic loop. We'll use the `pokedex` function we just created. The implementation we have above has some limitations:\n",
        "- Its a single turn, so if it fails to answer my question in one pass it is over.\n",
        "\n",
        "![](https://github.com/morganmcg1/deep-research-bot/blob/main/notebooks/images/05_agent.png?raw=1)\n",
        "\n",
        "From the really good [Anthropic Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents) article and encourage people to read it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1737003e",
      "metadata": {
        "id": "1737003e"
      },
      "source": [
        "A simple for loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63bc2975",
      "metadata": {
        "id": "63bc2975"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "def research_loop(query: str, max_turns: int = 4, tools = [exa_search, add_numbers]) -> str:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an agent that has access to an advanced search engine. Please provide the user with the information they are looking for by using the search tool provided. Make sure to keep the sources. Always use tools to obtain reliable results. Return the final answer in markdown format.\"},\n",
        "        {\"role\": \"user\", \"content\": query}]\n",
        "\n",
        "    for turn in range(max_turns):\n",
        "        console.rule(f\"Agent Loop Turn {turn + 1}/{max_turns}\")\n",
        "\n",
        "        # call model with tools\n",
        "        response = call_model(\n",
        "            model_name=MODEL_MEDIUM,\n",
        "            messages=messages,\n",
        "            tools=[t.tool_schema for t in tools])\n",
        "\n",
        "        # add the response to the messages\n",
        "        messages.append(response.model_dump())\n",
        "\n",
        "        # if the LLM requested tool calls, perform them\n",
        "        if response.tool_calls:\n",
        "            # perform the tool calls\n",
        "            tool_outputs = perform_tool_calls(tools=tools, tool_calls=response.tool_calls)\n",
        "            messages.extend(tool_outputs)\n",
        "        # LLM gave content response\n",
        "        elif response.content:\n",
        "            console.rule(\"Final Model Response\")\n",
        "            console.md(response.content)\n",
        "            return response.content\n",
        "        else:\n",
        "            print(\"LLM response had neither content nor tool calls. Stopping loop.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fecad3a4",
      "metadata": {
        "id": "fecad3a4"
      },
      "outputs": [],
      "source": [
        "res = research_loop(\"What is the sum of the populations of the 2 major EU cities?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60e589c3",
      "metadata": {
        "id": "60e589c3"
      },
      "source": [
        "# 5. Structuring the Agent with Classes\n",
        "\n",
        "The loop above works, but for more complex agents, encapsulating the logic and state within classes is much better. We'll define:\n",
        "- `AgentState`: A Pydantic model to hold the conversation history and potentially other state.\n",
        "- `SimpleAgent`: A class containing the agent's configuration (model, system message, tools) and logic (`step`, `run`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31f65412",
      "metadata": {
        "id": "31f65412"
      },
      "outputs": [],
      "source": [
        "class AgentState(BaseModel):\n",
        "    \"\"\"Manages the state of the agent.\"\"\"\n",
        "    messages: list[dict[str, Any]] = Field(default_factory=list)\n",
        "    step: int = Field(default=0)\n",
        "    final_assistant_content: str | None = None # Populated at the end of a run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26f57c8e",
      "metadata": {
        "id": "26f57c8e"
      },
      "outputs": [],
      "source": [
        "class SimpleAgent:\n",
        "    \"\"\"A simple agent class with tracing, state, and tool processing.\"\"\"\n",
        "    def __init__(self, model_name: str, system_message: str, tools: list[Callable], state_class: BaseModel = AgentState):\n",
        "        self.model_name = model_name\n",
        "        self.system_message = system_message\n",
        "        self.tools = [function_tool(t) for t in tools] # add schemas to the tools\n",
        "        self.state_class = state_class\n",
        "\n",
        "    @weave.op(name=\"SimpleAgent.step\") # Trace each step\n",
        "    def step(self, state: AgentState) -> AgentState:\n",
        "        step = state.step + 1\n",
        "        messages = state.messages\n",
        "        final_assistant_content = None\n",
        "        try:\n",
        "            # call model with tools\n",
        "            response = call_model(\n",
        "                model_name=self.model_name,\n",
        "                messages=messages,\n",
        "                tools=[t.tool_schema for t in self.tools])\n",
        "\n",
        "            # add the response to the messages\n",
        "            messages.append(response.model_dump())\n",
        "\n",
        "            # if the LLM requested tool calls, perform them\n",
        "            if response.tool_calls:\n",
        "                # perform the tool calls\n",
        "                tool_outputs = perform_tool_calls(tools=self.tools, tool_calls=response.tool_calls)\n",
        "                messages.extend(tool_outputs)\n",
        "\n",
        "            # LLM gave content response\n",
        "            else:\n",
        "                final_assistant_content = response.content\n",
        "        except Exception as e:\n",
        "            console.print(f\"ERROR in Agent Step: {e}\")\n",
        "            # Add an error message to history to indicate failure\n",
        "            messages.append({\"role\": \"assistant\", \"content\": f\"Agent error in step: {str(e)}\"})\n",
        "            final_assistant_content = f\"Agent error in step {step}: {str(e)}\"\n",
        "        return self.state_class(messages=messages, step=step, final_assistant_content=final_assistant_content)\n",
        "\n",
        "    @weave.op(name=\"SimpleAgent.run\")\n",
        "    def run(self, user_prompt: str, max_turns: int = 10, **state_kwargs: Any) -> AgentState:\n",
        "        state = self.state_class(messages=[\n",
        "            {\"role\": \"system\", \"content\": self.system_message},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}], **state_kwargs)\n",
        "        for _ in range(max_turns):\n",
        "            console.rule(f\"Agent Loop Turn {state.step+1}/{max_turns}\")\n",
        "            state = self.step(state)\n",
        "            if state.final_assistant_content:\n",
        "                return state\n",
        "        return state\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80842566",
      "metadata": {
        "id": "80842566"
      },
      "source": [
        "![](https://github.com/morganmcg1/deep-research-bot/blob/main/notebooks/images/07_simple_agent.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ce6b40a",
      "metadata": {
        "id": "6ce6b40a"
      },
      "outputs": [],
      "source": [
        "agent = SimpleAgent(\n",
        "    model_name=MODEL_SMALL,\n",
        "    system_message=\"You are an agent that has access to an advanced search engine. Please provide the user with the information they are looking for by using the search tool provided. Make sure to keep the sources. Always use tools to obtain reliable results. Return the final answer in markdown format.\",\n",
        "    tools=[exa_search, add_numbers],\n",
        ")\n",
        "state = agent.run(user_prompt=\"What is the combined weight of Ash's first 3 pokemons?\", max_turns=3)\n",
        "print(f\"Final response: {state.final_assistant_content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5947ac8b",
      "metadata": {
        "id": "5947ac8b"
      },
      "outputs": [],
      "source": [
        "@weave.op\n",
        "@function_tool # <- we can use the decorator to automatically generate the tool schema\n",
        "def exa_search_and_refine(query: str, num_results: int = 5) -> list[dict[str, str]]:\n",
        "    \"\"\"Perform a search query on the web and retrieve the most relevant URLs and web content.\n",
        "\n",
        "    This function uses the Exa search API to find relevant web pages based on the query\n",
        "    and returns their titles, text content, and URLs.\n",
        "\n",
        "    Args:\n",
        "        query: The search query. Use detailed, specific queries for better results.\n",
        "               The quality of results depends on the specificity of the query.\n",
        "        num_results: The number of search results to retrieve. Defaults to 5.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, each containing:\n",
        "            - title: The title of the web page\n",
        "            - text: The text content of the web page\n",
        "            - url: The URL of the web page\n",
        "    \"\"\"\n",
        "    search_results = exa_client.search_and_contents(query=query, type='auto', num_results=num_results)\n",
        "\n",
        "    @weave.op\n",
        "    def refine_search_result(result, query):\n",
        "        messages = [\n",
        "            {\"role\":\"system\", \"content\": f\"Your task is to extract from the search results only the info that is relevant to answer the query\"},\n",
        "            {\"role\": \"user\", \"content\": f\"- query: {query}\\n- Search result: {result}\"}\n",
        "        ]\n",
        "        refined_search = call_model(model_name=MODEL_SMALL, messages=messages)\n",
        "        return refined_search.content\n",
        "\n",
        "    output = []\n",
        "    for item, result in enumerate(search_results.results):\n",
        "        console.print(f\"Refining result {item+1}\")\n",
        "        refined_text = refine_search_result(result.text, query)\n",
        "        output.append(\n",
        "            {\"title\": result.title,\n",
        "            \"text\": refined_text,\n",
        "            \"url\": result.url\n",
        "            }\n",
        "        )\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81ce4165",
      "metadata": {
        "id": "81ce4165"
      },
      "outputs": [],
      "source": [
        "agent = SimpleAgent(\n",
        "    model_name=MODEL_SMALL,\n",
        "    system_message=\"You are an agent that has access to an advanced search engine. Please provide the user with the information they are looking for by using the search tool provided. Make sure to keep the sources. Always use tools to obtain reliable results. Return the final answer in markdown format.\",\n",
        "    tools=[exa_search_and_refine, add_numbers]\n",
        ")\n",
        "state = agent.run(user_prompt=\"What is the combined weight of Ash's first 3 pokemons?\", max_turns=3)\n",
        "print(f\"Final response: {state.final_assistant_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e9cd3c5",
      "metadata": {
        "id": "4e9cd3c5"
      },
      "source": [
        "Possible improvements to the SimpleAgent:\n",
        "- Give the model info about the state of the conversation, you could inject a message with the model context pressure, steps left, etc.\n",
        "- Structured output. Make the model output a specific format, for instance a JSON with the expected fields.\n",
        "- Add more tools like read and write files, access a database.\n",
        "- Agent handoff: Agent1 does triage and Agent2 executes specific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9313c612",
      "metadata": {
        "id": "9313c612"
      },
      "source": [
        "## Evaluating our SimpleAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6057c0ba",
      "metadata": {
        "id": "6057c0ba"
      },
      "outputs": [],
      "source": [
        "# Run this in colab\n",
        "\n",
        "# !git clone https://github.com/morganmcg1/deep-research-bot/\n",
        "# !mv deep-research-bot/data /data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64f60ffa",
      "metadata": {
        "id": "64f60ffa"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to Python path\n",
        "project_root = Path.cwd().parent\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c9f647e",
      "metadata": {
        "id": "5c9f647e"
      },
      "outputs": [],
      "source": [
        "from deep_research_bot.evaluation.eval import run_evaluation\n",
        "from deep_research_bot.evaluation.eval_config import EvalConfig\n",
        "\n",
        "MAX_TURNS=5\n",
        "\n",
        "agent = SimpleAgent(\n",
        "    model_name=MODEL_SMALL,\n",
        "    system_message=\"You are an agent that has access to an advanced search engine. Please provide the user with the information they are looking for by using the search tool provided. Make sure to keep the sources. Always use tools to obtain reliable results. Return the final answer in markdown format.\",\n",
        "    tools=[exa_search_and_refine, add_numbers],\n",
        ")\n",
        "\n",
        "eval_config = EvalConfig(\n",
        "    evaluation_name=f\"SimpleAgent_max-turns-{MAX_TURNS}_{agent.model_name.split('/')[-1]}\",\n",
        "    trials=2,\n",
        "    limit=20,\n",
        "    judge_model=\"deepseek-ai/DeepSeek-R1-0528\",\n",
        "    weave_parallelism=4,\n",
        "    queries=project_root / \"data/prompt_data/query.jsonl\",\n",
        "    reference=project_root / \"data/test_data/cleaned_data/reference.jsonl\",\n",
        "    criteria=project_root / \"data/criteria_data/criteria.jsonl\",\n",
        ")\n",
        "\n",
        "results = await run_evaluation(\n",
        "    eval_config=eval_config,\n",
        "    agent_callable=partial(agent.run, max_turns=MAX_TURNS),  # <- partial to limit the number of agent turns\n",
        ")\n",
        "results"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "87f31f0f80d34f0e80edb32303fc9f7f": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_8c62c91856aa4940b595d85c9fca758a",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "8c62c91856aa4940b595d85c9fca758a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}